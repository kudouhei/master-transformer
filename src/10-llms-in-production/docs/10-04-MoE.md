## Mixture of experts (MoE)

A mixture of experts (MoE) is functionally the same as any other model for training but contains a trick under the hood: sparsity. This gives the advantage of being able to train a bunch of models on a diverse set of data and tasks at once.

#### What is MoE?
Learn from [A Visual Guide to Mixture of Experts](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)

Mixture of Experts (MoE) is a technique that uses many different sub-models (or “experts”) to improve the quality of LLMs.
- Experts: Each FFNN layer now has a set of “experts” of which a subset can be chosen. These “experts” are typically FFNNs themselves.
- Router or gate network: Determines which tokens are sent to which experts.

The "experts" it learns syntactic information on a word level instead, more specifically, their expertise is in handling specific tokens in specific contexts.

![](../images/MoE-1.png)

The router (gate network) selects the expert(s) best suited for a given input: Each expert is not an entire LLM but a submodel part of an LLM’s architecture.

![](../images/MoE-2.png)

#### The experts
To explore what experts represent and how they work, let us first examine what MoE is supposed to replace; the dense layers.

**Dense layers**
Mixture of Experts (MoE) all start from a relatively basic functionality of LLMs, namely the `Feedforward Neural Network (FFNN)`.

Remember that a standard decoder-only Transformer architecture has the FFNN applied after layer normalization:

![](../images/MoE-3.png)

An FFNN allows the model to use the contextual information created by the attention mechanism, transforming it further to capture more complex relationships in the data.

The FFNN, however, does grow quickly in size. To learn these complex relationships, it typically expands on the input it receives:

![](../images/MoE-4.png)

This expansion is done by the FFNN, which is a matrix multiplication of the input with a weight matrix:

**Sparse Layers**
The FFNN in a traditional Transformer is called a dense model since all parameters (its weights and biases) are activated.

In contrast, sparse models only activate a portion of their total parameters and are closely related to Mixture of Experts.

![](../images/MoE-5.png)

The underlying idea is that each expert learns different information during training. Then, when running inference, only specific experts are used as they are most relevant for a given task.

When asked a question, we can select the expert best suited for a given task:

![](../images/MoE-6.png)

**What does an Expert Learn?**

![](../images/MoE-7.png)

Experts in decoder models, however, do not seem to have the same type of specialization. That does not mean though that all experts are equal.

**The Architecture of Experts**

![](../images/MoE-8.png)

Since most LLMs have several decoder blocks, a given text will pass through multiple experts before the text is generated:

![](../images/MoE-9.png)

The chosen experts likely differ between tokens which results in different “paths” being taken:

![](../images/MoE-10.png)

If we update our visualization of the decoder block, it would now contain more FFNNs (one for each expert) instead:

![](../images/MoE-11.png)

The decoder block now has multiple FFNNs (each an “expert”) that it can use during inference.

#### The Routing Mechanism
Now that we have a set of experts, how does the model know which experts to use? Just before the experts, a router (also called a gate network) is added which is trained to choose which expert to choose for a given token.

**The Router**
The router (or gate network) is also an FFNN and is used to choose the expert based on a particular input. It outputs probabilities which it uses to select the best matching expert:

![](../images/MoE-12.png)

The expert layer returns the output of the selected expert multiplied by the gate value (selection probabilities).

The router together with the experts (of which only a few are selected) makes up the MoE Layer:

![](../images/MoE-13.png)

A given MoE layer comes in two sizes, either a sparse or a dense mixture of experts.

Both use a router to select experts but a Sparse MoE only selects a few whereas a Dense MoE selects them all but potentially in different distributions.

![](../images/MoE-14.png)

For instance, given a set of tokens, a MoE will distribute the tokens across all experts whereas a Sparse MoE will only select a few experts.

In the current state of LLMs, when you see a “MoE” it will typically be a Sparse MoE as it allows you to use a subset of experts. This is computationally cheaper which is an important trait for LLMs.

**Selection of Experts**
The gating network is arguably the most important component of any MoE as it not only decides which experts to choose during inference but also training.

In its most basic form, we multiply the `input (x)` by the `router weight matrix (W)`:

![](../images/MoE-15.png)

Then, we apply a `SoftMax` on the output to create a probability distribution `G(x)` per expert:

![](../images/MoE-16.png)

The router uses this probability distribution to choose the best matching expert for a given input.

Finally, we multiply the output of each router with each selected expert and sum the results.

![](../images/MoE-17.png)

Let’s put everything together and explore how the input flows through the router and experts:

![](../images/MoE-18.png)

#### Load Balancing

To balance the importance of experts, we will need to look at the router as it is the main component to decide which experts to choose at a given time.

**KeepTopK**
One method of load balancing the router is through a straightforward extension called KeepTopK2. By introducing trainable (gaussian) noise, we can prevent the same experts from always being picked:

![](../images/MoE-19.png)

Then, all but the top k experts that you want activating (for example 2) will have their weights set to -∞:

![](../images/MoE-20.png)

By setting these weights to -∞, the output of the SoftMax on these weights will result in a probability of 0:

![](../images/MoE-21.png)

The KeepTopK strategy is one that many LLMs still use despite many promising alternatives. Note that KeepTopK can also be used without the additional noise.

**Token Choice**
The KeepTopK strategy routes each token to a few selected experts. This method is called Token Choice3 and allows for a given token to be sent to one expert (top-1 routing):

![](../images/MoE-22.png)

or to more than one expert (top-k routing):

![](../images/MoE-23.png)

A major benefit is that it allows the experts’ respective contributions to be weighed and integrated.

**Auxiliary Loss**
To get a more even distribution of experts during training, the auxiliary loss (also called load balancing loss) was added to the network’s regular loss.
It adds a constraint that forces experts to have equal importance.

The first component of this auxiliary loss is to sum the router values for each expert over the entire batch:

![](../images/MoE-24.png)

This gives us the importance scores per expert which represents how likely a given expert will be chosen regardless of the input.

We can use this to calculate the coefficient variation (CV), which tells us how different the importance scores are between experts.

![](../images/MoE-25.png)

For instance, if there are a lot of differences in importance scores, the CV will be high:

![](../images/MoE-26.png)

In contrast, if all experts have similar importance scores, the CV will be low (which is what we aim for):

![](../images/MoE-27.png)

Using this CV score, we can update the auxiliary loss during training such that it aims to lower the CV score as much as possible (thereby giving equal importance to each expert):

![](../images/MoE-28.png)

Finally, the auxiliary loss is added as a separate loss to optimize during training.

**Expert Capacity**
Imbalance is not just found in the experts that were chosen but also in the distributions of tokens that are sent to the expert.

For instance, if input tokens are disproportionally sent to one expert over another then that might also result in undertraining:

A solution to this problem is to limit the amount of tokens a given expert can handle, namely `Expert Capacity`. By the time an expert has reached capacity, the resulting tokens will be sent to the next expert:

![](../images/MoE-29.png)

If both experts have reached their capacity, the token will not be processed by any expert but instead sent to the next layer. This is referred to as token overflow.

![](../images/MoE-30.png)



