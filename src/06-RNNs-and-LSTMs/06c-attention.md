# Attention

The simplicity of the encoder-decoder model is its clean separation of the encoder-which builds a representation of the source textâ€”from the decoder, which uses this context to generate a target text.

In the model as weâ€™ve described it so far, this context vector is $h_n$, the hidden state of the last ($n^{th}$ ) time step of the source text. 

This final hidden state is thus acting as a **bottleneck**: `it must represent absolutely everything about the meaning of the source text, since the only thing the decoder knows about the source text is whatâ€™s in this context vector.`Information at the beginning of the sentence, especially for long sentences, may not be equally well represented in the context vector.

![Attention](./images/15-bottleneck.png)

## Attention mechanism

The attention mechanism is a solution to the bottleneck problem, a way of allowing the decoder to get information from all the hidden states of the encoder, not just the last hidden state.

In the attention mechanism, as in the vanilla encoder-decoder model, the context vector $c$ is a single vector that is a function of the hidden states of the encoder. But instead of being taken from the last hidden state, itâ€™s a weighted average of **all** the hidden states of the decoder.

And this weighted average is also informed by part of the decoder state as well, the state of the decoder right before the current token $i$.

That is, $c = f(h_1^e, \cdots, h_n^e, h_{i-1}^d)$.

Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, but also informed by and hence different for each token in decoding.

This context vector, $c_i$, is generated anew with each decoding step $i$ and takes all of the encoder hidden states into account in its derivation.

We then make this context available during decoding by conditioning the computation of the current decoder hidden state on it (along with the prior hidden state and the previous output generated by the decoder), as we see in this equation:

$$ h_i^d = g(\hat{y}_{i-1}, h_{i-1}^d, c_i) $$

![Attention](./images/16-attention.png)

- The first step in computing $c_i$ is to compute how much to focus on each encoder state, how relevant each encoder state is to the decoder state captured in $h_{i-1}^d$.
- We capture relevance by computingâ€” at each state $i$ during decodingâ€”a score $(h_{j}^e, h_{i-1}^d)$ for each encoder state $j$.

## Dot-product attention

The simplest such score, called dot-product attention, implements relevance as similarity: measuring how similar the decoder hidden state is to an encoder hidden state, by computing the dot product between them:

$$ \text{score}(h_j^e, h_{i-1}^d) = h_j^e \cdot h_{i-1}^d $$

The score that results from this dot product is a scalar that reflects the degree of similarity between the two vectors. The vector of these scores across all the encoder hidden states gives us the relevance of each encoder state to the current step of the decoder.

To make use of these scores, weâ€™ll normalize them with a **softmax** to create a vector of weights, $\alpha_{ij}$, that tells us the proportional relevance of each encoder hidden state $j$ to the prior hidden decoder state, $h_{i-1}^d$.

$$ \alpha_{ij} = \text{softmax}(\text{score}(h_j^e, h_{i-1}^d)) = \frac{\exp(\text{score}(h_j^e, h_{i-1}^d))}{\sum_{k=1}^n \exp(\text{score}(h_k^e, h_{i-1}^d))} $$

Finally, given the distribution in $\alpha$, we can compute a fixed-length context vector for the current decoder state by taking a weighted average over all the encoder hidden states.

$$ c_i = \sum_{j=1}^n \alpha_{ij} h_j^e $$

With this, we finally have a fixed-length context vector that takes into account information from the entire encoder state that is dynamically updated to reflect the needs of the decoder at each step of decoding.

![Attention](./images/17-coder-attention.png)

Itâ€™s also possible to create more sophisticated scoring functions for attention models. Instead of simple dot product attention, we can get a more powerful function that computes the relevance of each encoder hidden state to the decoder hidden state by parameterizing the score with its own set of weights, $W_s$.

$$ \text{score}(h_j^e, h_{i-1}^d) = h_j^e \cdot W_s \cdot h_{i-1}^d $$

The weights $W_s$, which are then trained during normal end-to-end training, give the network the ability to learn which aspects of similarity between the decoder and encoder states are important to the current application. 

This bilinear model also allows the encoder and decoder to use different dimensional vectors, whereas the simple dot-product attention requires that the encoder and decoder hidden states have the same dimensionality. 

**Note:**
**å…³é”®ç»„ä»¶è¯¦è§£**
1. ç¼–ç å™¨ï¼ˆEncoderï¼‰
   - è¾“å…¥åºåˆ— ğ‘¥1, ğ‘¥2, . . . , ğ‘¥n è¢«é€å…¥ç¼–ç å™¨çš„éšè—å±‚ï¼ˆé€šå¸¸æ˜¯ RNNã€LSTM æˆ– Transformer çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚
   - ç¼–ç å™¨è¾“å‡ºçš„æ˜¯æ¯ä¸ªæ—¶åˆ»çš„éšè—çŠ¶æ€ $h_j^e$ï¼Œå…¶ä¸­ $j=1,2,...,n$ã€‚

2. æ³¨æ„åŠ›æœºåˆ¶ï¼ˆAttentionï¼‰
   - è§£ç å™¨åœ¨ç”Ÿæˆç¬¬ $i$ ä¸ªè¾“å‡º $y_i$ æ—¶ï¼Œä¼šè®¡ç®—ä¸€ä¸ªä¸Šä¸‹æ–‡å‘é‡ $c_i$ã€‚
   - ä¸Šä¸‹æ–‡å‘é‡ $c_i$ æ˜¯æ‰€æœ‰ç¼–ç å™¨éšè—çŠ¶æ€çš„åŠ æƒå’Œ: $c_i = \sum_{j=1}^n \alpha_{ij} h_j^e$
     - $\alpha_{ij}$ æ˜¯æ³¨æ„åŠ›æƒé‡ï¼Œè¡¨ç¤ºåœ¨ç¬¬ $i$ ä¸ªè§£ç æ­¥éª¤ä¸­ï¼Œç¼–ç å™¨ç¬¬ $j$ ä¸ªéšè—çŠ¶æ€å¯¹å½“å‰è¾“å‡ºçš„â€œå…³æ³¨åº¦â€ã€‚
     - æ³¨æ„åŠ›æƒé‡æ˜¯é€šè¿‡è§£ç å™¨ä¸Šä¸€ä¸ªéšè—çŠ¶æ€ $h_{i-1}^d$ å’Œæ¯ä¸€ä¸ªç¼–ç å™¨éšè—çŠ¶æ€ $h_j^e$ çš„æ‰“åˆ†å‡½æ•°ï¼ˆé€šå¸¸æ˜¯ç‚¹ç§¯æˆ–å…¶ä»–ï¼‰è®¡ç®—å‡ºæ¥çš„
     - åœ¨å›¾ä¸­ï¼š
       - è™šçº¿ç®­å¤´è¡¨ç¤ºæ‰“åˆ†è¿‡ç¨‹, å³è®¡ç®— $\alpha_{ij}$
       - å®çº¿ç®­å¤´è¡¨ç¤ºåŠ æƒæ±‚å’Œè¿‡ç¨‹, å³è®¡ç®— $c_i$
       - å›¾ä¸­ç»¿è‰²åœ†åœˆä¸­çš„æ•°å­—ï¼ˆä¾‹å¦‚ 0.4ã€0.3ã€0.1ã€0.2ï¼‰æ˜¯å…·ä½“çš„æ³¨æ„åŠ›æƒé‡ $\alpha_{ij}$

3. è§£ç å™¨ï¼ˆDecoderï¼‰
   - è§£ç å™¨åœ¨ç”Ÿæˆç¬¬ $i$ ä¸ªè¾“å‡º $y_i$ æ—¶
     - è¾“å…¥åŒ…æ‹¬å‰ä¸€ä¸ªæ—¶åˆ»çš„éšè—çŠ¶æ€ $h_{i-1}^d$ å’Œå‰ä¸€ä¸ªè¾“å‡º $y_{i-1}$ï¼Œ ä»¥åŠä¸Šä¸‹æ–‡å‘é‡ $c_i$
     - è§£ç å™¨ä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥è®¡ç®—å½“å‰æ—¶åˆ»çš„éšè—çŠ¶æ€ $h_i^d$ï¼Œå¹¶ç”Ÿæˆå½“å‰æ—¶åˆ»çš„è¾“å‡º $y_i$



