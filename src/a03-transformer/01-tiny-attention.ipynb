{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import math\n",
    "\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTConfig:\n",
    "  block_size: int = 32 # max_seq_len: what is the maximum context length for predictions?\n",
    "  batch_size: int = 16 # how many independent sequences will we process in parallel?\n",
    "  n_layer: int = 4\n",
    "  n_head: int = 4 # multi-head\n",
    "  n_embd: int = 64 # n_embd: hidden_dim, hiden_size 词向量，位置向量，以及内部特征向量的维数\n",
    "  head_size: int = n_embd // n_head\n",
    "  dropout: float = 0.1\n",
    "  vocab_size: int = 50257 # tiktoken 使用的是 GPT-2 的词表，大约有 50257 个token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class singleHeadAttention(nn.Module):\n",
    "    def __init__(self, config: GPTConfig):\n",
    "        super().__init__()\n",
    "        # key, query, value projections for all heads\n",
    "        # nn.Linear(n,m) is a module that creates single layer feed forward network with n inputs and m output.\n",
    "        self.key = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.value = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.query = nn.Linear(config.n_embd, config.head_size)\n",
    "        self.head_size = config.head_size\n",
    "\n",
    "        # attention_mask using register_buffer\n",
    "        self.register_buffer(\n",
    "            'attention_mask',\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "        )\n",
    "        # Dropout是一种常用的正则化方法，通过随机将部分神经元的输出置为0来减少过拟合\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_size = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        k = self.key(x) # (batch_size, seq_len, head_size)\n",
    "        v = self.value(x) # (batch_size, seq_len, head_size)\n",
    "        q = self.query(x) # (batch_size, seq_len, head_size)  \n",
    "\n",
    "        # transpose the second last and last dimensions of tensor k\n",
    "        # 除以sqrt(head_size) before softmax\n",
    "        weight = weight.masked_fill(\n",
    "            self.attention_mask[:seq_len, :seq_len] == 0,\n",
    "            float('-inf')\n",
    "        ) / math.sqrt(self.head_size) # 这里的 hidden_size 其实是 head_size，因为是单头\n",
    "        weight = F.softmax(weight, dim=-1)\n",
    "        weight = self.dropout(weight)\n",
    "        out = weight @ v\n",
    "        return out\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.heads = nn.ModuleList(\n",
    "        [\n",
    "            SingleHeadAttention(config)\n",
    "            for _ in range(config.n_head)\n",
    "        ]\n",
    "    )\n",
    "    self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "    self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = torch.cat(\n",
    "        [ h(x) for h in self.heads],\n",
    "         dim = -1\n",
    "     )\n",
    "    output = self.proj(output)\n",
    "    output = self.dropout(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    # 实际上就是 MLP\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
