{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USGV9gqwLX-M",
        "outputId": "a3a96f15-c24e-4df1-f59b-5ea8ddd3a578"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-23 13:39:06--  https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/gutenberg/austen-emma.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 887071 (866K) [text/plain]\n",
            "Saving to: ‘austen-emma.txt’\n",
            "\n",
            "austen-emma.txt     100%[===================>] 866.28K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-03-23 13:39:06 (14.7 MB/s) - ‘austen-emma.txt’ saved [887071/887071]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !wget https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/gutenberg/austen-emma.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_NmeaxD9Oqnn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the BPE tokenizer from the tokenizers library\n",
        "\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFKC, Sequence, Lowercase\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.trainers import BpeTrainer"
      ],
      "metadata": {
        "id": "slQtVR0BO5DW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase normalization\n",
        "\n",
        "# makes a tokenizer from the BPE tokenizer class\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.normalizer = Sequence([\n",
        "    Lowercase()\n",
        "])\n",
        "\n",
        "# For the normalization part, Lowercase has been added,\n",
        "# and the pre_tokenizer attribute is set to ByteLevel to ensure we have bytes as our input.\n",
        "# The decoder attribute must be also set to ByteLevelDecoder to be able to decode correctly.\n",
        "\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "tokenizer.decoder = ByteLevelDecoder()"
      ],
      "metadata": {
        "id": "tNkq3jvmPV4h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer be trained using a 50000 maximum vocabulary size and an initial alphabet from ByteLevel\n",
        "\n",
        "trainer = BpeTrainer(vocab_size=50000, inital_alphabet=ByteLevel.alphabet(),\n",
        "                     special_tokens=[\n",
        "                        \"<s>\",\n",
        "                        \"<pad>\",\n",
        "                        \"</s>\",\n",
        "                        \"<unk>\",\n",
        "                        \"<mask>\"\n",
        "                    ])\n",
        "tokenizer.train([\"austen-emma.txt\"], trainer)"
      ],
      "metadata": {
        "id": "VUI8XX8JP3f_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir tokenizer_gpt"
      ],
      "metadata": {
        "id": "PI7dTC9cS6We"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save(\"tokenizer_gpt/tokenizer.json\")"
      ],
      "metadata": {
        "id": "IeGO88qFS7sK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2TokenizerFast, GPT2Config, TFGPT2LMHeadModel"
      ],
      "metadata": {
        "id": "0tHQfOEvTDDp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_gpt = GPT2TokenizerFast.from_pretrained(\"tokenizer_gpt\")\n"
      ],
      "metadata": {
        "id": "5bGkEIubTIXQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_gpt.add_special_tokens({\n",
        "  \"eos_token\": \"</s>\",\n",
        "  \"bos_token\": \"<s>\",\n",
        "  \"unk_token\": \"<unk>\",\n",
        "  \"pad_token\": \"<pad>\",\n",
        "  \"mask_token\": \"<mask>\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRtNvcUXTSz3",
        "outputId": "242c95e5-08a0-46ea-cc51-6a29db33d1d6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_gpt.eos_token_id\n",
        "\n",
        "# This code will output the End-of-Sentence (EOS) token Identifier (ID)\n",
        "#, which is 2 for the current tokenizer."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tH-1Vd8TYwO",
        "outputId": "f2ce22fe-e6cd-4fa2-8163-c23f428e201c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_gpt.encode(\"<s> this is </s>\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khHdR-oFTgC5",
        "outputId": "39700665-fd43-47d6-b54a-af8741180160"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 265, 157, 56, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = GPT2Config(\n",
        "  vocab_size=tokenizer_gpt.vocab_size,\n",
        "  bos_token_id=tokenizer_gpt.bos_token_id,\n",
        "  eos_token_id=tokenizer_gpt.eos_token_id\n",
        ")\n",
        "model = TFGPT2LMHeadModel(config)"
      ],
      "metadata": {
        "id": "Qb85mjX_Th8c"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcplJK3sTlFJ",
        "outputId": "8196dfb7-e006-484b-d97e-773455d01d98"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Config {\n",
              "  \"activation_function\": \"gelu_new\",\n",
              "  \"attn_pdrop\": 0.1,\n",
              "  \"bos_token_id\": 0,\n",
              "  \"embd_pdrop\": 0.1,\n",
              "  \"eos_token_id\": 2,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"layer_norm_epsilon\": 1e-05,\n",
              "  \"model_type\": \"gpt2\",\n",
              "  \"n_embd\": 768,\n",
              "  \"n_head\": 12,\n",
              "  \"n_inner\": null,\n",
              "  \"n_layer\": 12,\n",
              "  \"n_positions\": 1024,\n",
              "  \"reorder_and_upcast_attn\": false,\n",
              "  \"resid_pdrop\": 0.1,\n",
              "  \"scale_attn_by_inverse_layer_idx\": false,\n",
              "  \"scale_attn_weights\": true,\n",
              "  \"summary_activation\": null,\n",
              "  \"summary_first_dropout\": 0.1,\n",
              "  \"summary_proj_to_labels\": true,\n",
              "  \"summary_type\": \"cls_index\",\n",
              "  \"summary_use_proj\": true,\n",
              "  \"transformers_version\": \"4.49.0\",\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 11750\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"austen-emma.txt\", \"r\", encoding='utf-8') as f:\n",
        "    content = f.readlines()"
      ],
      "metadata": {
        "id": "GC4w9NNjTo5Q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_p = []\n",
        "for c in content:\n",
        "    if len(c)>10:\n",
        "        content_p.append(c.strip())"
      ],
      "metadata": {
        "id": "avtCBTA5TrOM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content_p = \" \".join(content_p)+tokenizer_gpt.eos_token"
      ],
      "metadata": {
        "id": "8y6WaVHhTs2z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenized_content = tokenizer_gpt.encode(content_p)"
      ],
      "metadata": {
        "id": "dErv2z3GTuIn"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []\n",
        "block_size = 100\n",
        "BATCH_SIZE = 12\n",
        "BUFFER_SIZE = 1000\n",
        "for i in range(0, len(tokenized_content)):\n",
        "    examples.append(tokenized_content[i:i + block_size])"
      ],
      "metadata": {
        "id": "aSdzccCATwRg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "labels = []\n",
        "for example in examples:\n",
        "    train_data.append(example[:-1])\n",
        "    labels.append(example[1:])"
      ],
      "metadata": {
        "id": "_wHe7ON_TyXo"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change 1000 if you want to train on full data\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_data[:1000], labels[:1000]))\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "wZrpJhQqT18e"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n"
      ],
      "metadata": {
        "id": "7ps1OwrKT3-f"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
      ],
      "metadata": {
        "id": "Q2QXfoegT5kk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n"
      ],
      "metadata": {
        "id": "zP42yTKyT6q3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])\n"
      ],
      "metadata": {
        "id": "q5fqZyuaT6ss"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# increase number of epochs for higher accuracy and lower loss\n",
        "\n",
        "num_epoch = 1\n",
        "history = model.fit(dataset, epochs=num_epoch)"
      ],
      "metadata": {
        "id": "-Jf9T885T9cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(start):\n",
        "    input_token_ids = tokenizer_gpt.encode(start, return_tensors='tf')\n",
        "    output = model.generate(\n",
        "        input_token_ids,\n",
        "        max_length = 10,\n",
        "        num_beams = 5,\n",
        "        temperature = 0.7,\n",
        "        no_repeat_ngram_size=2,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return tokenizer_gpt.decode(output[0])"
      ],
      "metadata": {
        "id": "wsJQcz_UUCae"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "4N9UYeRkUD38",
        "outputId": "6d172d7b-a61b-407b-f115-2fc9f8115db0"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' , the,, her her, a,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"wetson was very good\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "83XTyJ53UD2X",
        "outputId": "5798638c-51fc-4dd9-eb14-93c979e3e8d1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'wetson was very good,, her,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir my_gpt-2"
      ],
      "metadata": {
        "id": "8CDqXh-jUIK3"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"my_gpt-2/\")"
      ],
      "metadata": {
        "id": "wWqQ29LEUIsO"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_reloaded = TFGPT2LMHeadModel.from_pretrained(\"my_gpt-2/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ThAvasuUJzr",
        "outputId": "12b06918-6506-47ac-d942-f4745e69b1bb"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at my_gpt-2/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WEIGHTS_NAME, CONFIG_NAME, TF2_WEIGHTS_NAME, AutoModel, AutoTokenizer\n"
      ],
      "metadata": {
        "id": "m8FbsdcgULmZ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_gpt.save_pretrained(\"tokenizer_gpt_auto/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUPlGtJ0UMp2",
        "outputId": "fc2e5126-91fa-48d0-8349-34626f2f009d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tokenizer_gpt_auto/tokenizer_config.json',\n",
              " 'tokenizer_gpt_auto/special_tokens_map.json',\n",
              " 'tokenizer_gpt_auto/vocab.json',\n",
              " 'tokenizer_gpt_auto/merges.txt',\n",
              " 'tokenizer_gpt_auto/added_tokens.json',\n",
              " 'tokenizer_gpt_auto/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModel.from_pretrained(\"my_gpt-2/\", from_tf = True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_gpt_auto\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBTuny1lUOQ1",
        "outputId": "d4193ce1-e28a-4afe-f807-c0def29798d2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All TF 2.0 model weights were used when initializing GPT2Model.\n",
            "\n",
            "All the weights of GPT2Model were initialized from the TF 2.0 model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2Model for predictions without further training.\n"
          ]
        }
      ]
    }
  ]
}