## Large Language Models with Transformers

### Conditional generation

Conditional generation is the task of generating text conditioned on an input piece of text. That is, we give the LLM an input piece of text, generally called a prompt, and then have the LLM continue generating text token by token, conditioned on the prompt and the previously generated tokens.

**So why should we care about predicting upcoming words or tokens?**

The insight of large language modeling is that **many practical NLP tasks can be cast as word prediction**, and that a powerful-enough language model can solve them with a high degree of accuracy.

### Text summarization

Conditional generation can even be used to accomplish tasks that must generate longer responses. Consider the task of text summarization, which is to take a long text, such as a full-length article, and produce an effective shorter summary of it.


### Greedy decoding
Which words do we generate at each step?

- One simple way to generate words is to always generate the most likely word given the context.

Generating the most likely word given the context is called **greedy decoding**.

Thus in greedy decoding, at each time step in generation, the output $y_t$ is chosen by computing the probability for each possible output (every word in the vocabulary) and then choosing the highest probability word (the argmax):

$$y_t = \text{argmax}_{y} P(y_{t+1} | y_1, \ldots, y_t)$$

In practice, however, we donâ€™t use greedy decoding with large language models.

- A major problem with greedy decoding is that because the words it chooses are (by definition) extremely predictable, the resulting text is generic and often quite repetitive.










